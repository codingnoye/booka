# -*- coding: utf-8 -*-
"""Basic latent factor model with flybook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZPz2OsMGXZchczpdxwfl2O8fM_N3lE78
"""

import numpy as np
import pandas as pd
import matplotlib as plt
import torch
from sklearn.model_selection import train_test_split
import torch.nn.functional as F
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

cd/content/gdrive/My Drive/Recommed System

book_path = 'book_info_used.csv'
rating_path = 'booka_backend_review.csv'

req_cols = ['user_id', 'book_id', 'score']
req_cols1 = ['id', 'title', 'author']

rating = pd.read_csv(rating_path,usecols=req_cols, escapechar='\\') 
books = pd.read_csv(book_path,usecols=req_cols1,escapechar='\\') 

rating

def load_node_csv(rating, index_col):
    rating=pd.read_csv('booka_backend_review.csv',index_col=index_col)
    mapping = {index: i for i, index in enumerate(rating.index.unique())}
    return mapping
user_mapping=load_node_csv(rating,index_col='user_id')
book_mapping=load_node_csv(rating,index_col='book_id')
a=rating.groupby('score')
a.size()
print(len(book_mapping))

def load_edge_csv(rating, src_index_col, src_mapping, dst_index_col, dst_mapping,link_index_col,rating_threshold):

  df = rating
 
  edge_index = None
  src = [src_mapping[index] for index in df[src_index_col]]
  dst = [dst_mapping[index] for index in df[dst_index_col]]
  edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold
  print(edge_attr)
  edge_index = [[], []]
  for i in range(edge_attr.shape[0]):
    if(edge_attr[i]):
      edge_index[0].append(src[i])
      edge_index[1].append(dst[i])

  return torch.tensor(edge_index)
edge_index = load_edge_csv(
    rating,
    src_index_col='user_id',
    src_mapping=user_mapping,
    dst_index_col='book_id',
    dst_mapping=book_mapping,
    link_index_col='score',
    rating_threshold=7
  
)
edge_attr = torch.from_numpy(rating['score'].values).view(-1, 1).to(torch.long)
a=edge_index[1].tolist()
edge_attr
a=set(a)
a=list(a)
len(a)
print(len(edge_index[1]))

num_users, num_books = len(user_mapping), len(book_mapping)
print(num_users)
num_interactions = edge_index.shape[1]

all_indices = [i for i in range(num_interactions)]

train_indices, test_indices = train_test_split(
    all_indices, test_size=0.2, random_state=1)
train_edge_index = edge_index[:, train_indices]

test_edge_index = edge_index[:, test_indices]

train_edge_index

P = torch.randn(num_users, 3, requires_grad=True)
Q = torch.randn(num_books, 3, requires_grad=True)
bu = torch.randn(num_users, requires_grad=True) 
bi = torch.randn(num_books, requires_grad=True) 
optimizer = torch.optim.Adam([P, Q, bu, bi], lr= 0.1)

p_reg=0.0001
q_reg=0.0001
bi_reg=0.0001
bu_reg=0.0001

mean = (edge_attr[train_indices].sum()/edge_attr[train_indices].shape[0]).item()
X=[]
Y=[]
Y_test=[]
print(mean)

edge_attr[train_indices].shape
a=edge_attr[train_indices].view(-1)
a.float()
a.type()

# 각 유저가 본 책의 집합 
def get_user_positive_items(edge_index):
    user_pos_items = {}
    for i in range(edge_index.shape[1]):
        user = edge_index[0][i].item()
        item = edge_index[1][i].item()
        if user not in user_pos_items:
            user_pos_items[user] = []
        user_pos_items[user].append(item)
    return user_pos_items

def RecallPrecision_ATk(groundTruth, r, k):
    
    num_correct_pred = torch.sum(r, dim=-1)  
    
    user_num_liked = torch.Tensor([len(groundTruth[i])
                                  for i in range(len(groundTruth))])
    recall = torch.mean(num_correct_pred / user_num_liked)
    precision = torch.mean(num_correct_pred) / k
    return recall.item(), precision.item()

def get_metrics(P,Q,edge_index, exclude_edge_indices, k):
    
    user_embedding = P
    item_embedding = Q

    
    rating = torch.matmul(user_embedding, item_embedding.T)

    for exclude_edge_index in exclude_edge_indices:
        
        user_pos_items = get_user_positive_items(exclude_edge_index)
        
        exclude_users = []
        exclude_items = []
        for user, items in user_pos_items.items():
            exclude_users.extend([user] * len(items))
            exclude_items.extend(items)

        rating[exclude_users, exclude_items] = -(1 << 10)


    _, top_K_items = torch.topk(rating, k=k)

   
    users = edge_index[0].unique()

    test_user_pos_items = get_user_positive_items(edge_index)


    test_user_pos_items_list = [
        test_user_pos_items[user.item()] for user in users]

    
    r = []
    for user in users:
        ground_truth_items = test_user_pos_items[user.item()]
        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))
        r.append(label)
    r = torch.Tensor(np.array(r).astype('float'))

    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)

    return recall, precision

for epoch in range(1001):

    hypothesis = torch.sum(P[train_edge_index[0]] * Q[train_edge_index[1]], dim=1) + mean + bu[train_edge_index[0]] + bi[train_edge_index[1]]
    
    rmse = F.mse_loss(hypothesis , edge_attr[train_indices].view(-1).float())**0.5
    
    loss = rmse + p_reg*torch.sum(P**2) + q_reg*torch.sum(Q**2)+bi_reg*torch.sum(bi**2) + bu_reg*torch.sum(bu**2)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:

      hypo_test = torch.sum(P[test_edge_index[0]] * Q[test_edge_index[1]], dim=1)+ mean + bu[test_edge_index[0]] + bi[test_edge_index[1]]
      test_rmse = F.mse_loss(hypo_test, edge_attr[test_indices])**0.5
      
      X.append(epoch)
      Y.append(rmse)
      Y_test.append(test_rmse)
      print("epoch: {}, train_rmse: {}, test_rmse:{}" .format(epoch, rmse, test_rmse))

recall,precise=get_metrics(P,Q,test_edge_index,[train_edge_index],15)
print("recall:{}, pre:{}".format(recall,precise))

new_user_mapping={}
for k,v in user_mapping.items():
  new_user_mapping[v]=k 
new_user_mapping
book=edge_index[1].tolist()
book=set(book)
book=list(book)

bookid_title = pd.Series(books.title.values,index=books.id).to_dict() #id 인덱스 책 딕셔너리
bookid_genres = pd.Series(books.author.values,index=books.id).to_dict()

def make_prediction(user_id,num_rec):
  book=edge_index[1].tolist()
  book=set(book)
  book=list(book)

  user_pos_items=get_user_positive_items(edge_index)

  predict_user=torch.sum(P[user_mapping[user_id]]*Q[book],dim=1)+ mean
  + bu[user_mapping[user_id]] + bi[book] #book이 0인덱스 순서로 정렬 되어있어서 얘도그럼




  

  pred_df=predict_user
  pred_df=pred_df.detach().numpy()
  pred_df=pd.DataFrame(pred_df).astype("float")
  pred_df.columns=['score']

  a=user_pos_items[user_mapping[user_id]]
  new_book_mapping={}
  for k,v in book_mapping.items():
   new_book_mapping[v]=k 
  zin=[new_book_mapping[i] for i in book]

  pred_df.index= zin #인덱스를 책id로 변경
  pred_df_in = pred_df.sort_values(by='score', ascending=False).head(len(user_pos_items[user_mapping[user_id]]))
  pred_df_not = pred_df.sort_values(by='score', ascending=False).head(len(user_pos_items[user_mapping[user_id]])+num_rec)
  print(pred_df_in)
  
  index_list=pred_df_in.index
  index_list=index_list.tolist()

  not_index_list=pred_df_not.index
  not_index_list=not_index_list.tolist()

  not_index_list=set(not_index_list)-set(index_list)


  
  
  print(f"Here are some books read by {user_id}  user")
  pos_book = [index for index in user_pos_items[user_mapping[user_id]]] #해당 유저가 읽은 0인덱스
 
  book_ids = [list(book_mapping.keys())[list(book_mapping.values()).index(book)] for book in pos_book] #id 인덱스
  titles = [bookid_title[id] for id in book_ids]
  genres = [bookid_genres[id] for id in book_ids]
  for i in range(len(titles)):
    print(f"title: {titles[i]}, genres: {genres[i]} ")  

  print()

  titles = [bookid_title[id] for id in not_index_list]
  genres = [bookid_genres[id] for id in not_index_list]
  print(f"Here are some suggested book for user {user_id}")
  for i in range(num_rec):
    print(f"title: {titles[i]}, genres: {genres[i]} ")

user=edge_index[0].tolist()
user=set(user)
user=list(user)
zin2=[new_user_mapping[i] for i in user]

count=0
for i in zin2:
  USER_ID = i
  NUM_RECS = 15
  make_prediction(USER_ID,NUM_RECS)
  print("\n")
  count+=1
  if(count==10): break

