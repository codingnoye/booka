# -*- coding: utf-8 -*-
"""keybert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-o1sgh0c5INU--Rk3j-HBRGz523-fnuo
"""

!pip install sentence_transformers
!pip install konlpy
!pip install -U -q PyDrive
!pip install pymysql

import numpy as np
import itertools
import pandas as pd
from konlpy.tag import Okt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import torch
import torch.nn as nn
import re
import pymysql

from google.colab import drive
drive.mount('/content/gdrive')

cd/content/gdrive/My Drive/keybert

if torch.cuda.is_available():
  DEVICE=torch.device('cuda')
else:
  DEVICE=torch.device('cpu')

model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens').to(DEVICE)

conn = pymysql.connect(host='mm.noye.work', user='blueturtle', password='flyturtle!', db='booka', charset='utf8mb4')
curs = conn.cursor(pymysql.cursors.DictCursor)
book_path ="Select id,title,intro,`desc`,`desc_pub`,`desc_index` From backend_book"
curs.execute(book_path)
mc=curs.fetchall()
books=pd.DataFrame(mc)

#book_path = 'booka_backend_book.csv'
#req_cols=['id','title','intro','desc','desc_pub','desc_index'] 
#books = pd.read_csv(book_path,usecols=req_cols,escapechar='\\') 
books=books.dropna(how='all')
books=books.fillna('') # nan 값 바꿔주기 
books=books[books['title'].map(len)+books['desc_pub'].map(len)+books['intro'].map(len)+books['desc'].map(len)++books['desc_index'].map(len) > 100]


books=books.reset_index()
books

okt = Okt()
cos = nn.CosineSimilarity(dim=1, eps=1e-6)

def keyword_extraction(doc,top_n):

  tokenized_doc = okt.pos(doc)
  tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])
  n_gram_range = (1,1)

  count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])
  candidates = count.get_feature_names_out()

  doc_embedding = model.encode([doc])
  candidate_embeddings = model.encode(candidates)
  
  
  doc_embedding=torch.from_numpy(doc_embedding)
  candidate_embeddings=torch.from_numpy(candidate_embeddings)
  
  doc_embedding=doc_embedding.to(DEVICE)
  candidate_embeddings=candidate_embeddings.to(DEVICE)
  
  distances = cos(doc_embedding, candidate_embeddings)
  
  keywords = [candidates[index] for index in distances.argsort()[-top_n:]]
  
  return keywords

book_key=dict()

from tqdm import tqdm  #키워드 추출
for i in tqdm(range(len(books))):
  doc=books['desc_index'][i]+books['title'][i]+books['intro'][i]+books['desc'][i]+books['desc_pub'][i]
  if(len(re.sub('[^가-힣]', '', doc))>30):
    keyword=keyword_extraction(doc,8)
    book_key[books['id'][i]]=keyword
  else:
    continue

key_v=list(book_key.values())

from collections import Counter # 상위 n개 키워드 출력
key_extend=[]
for i in range(len(key_v)):
  key_extend.extend(key_v[i])
count=Counter(key_extend)
count_200=count.most_common(n=200)
dict_count_200=dict((x,y)for x,y in count_200 )
dict_count_200
count_200

import pickle

with open('book_key.pkl','wb') as f:
  pickle.dump(book_key,f)
with open('book_key.pkl','rb') as f:
  test = pickle.load(f)

##############################################################################

import matplotlib.pyplot as plt
import numpy as np

x = np.arange(200) # 그래프
word = dict_count_200.keys()
values = dict_count_200.values()

plt.figure(figsize=(40,8))

plt.bar(x, values)
plt.xticks(x, word)

plt.show()

def get_key(val): #카테고리 별 id
  matching_word=[]
  for key, value in book_key.items():
    for i in value:
      if val == i:
        matching_word.append(key)
  return matching_word

result=get_key(count_200[0][0]) 
result

##################################################################################################

def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):
    # 문서와 각 키워드들 간의 유사도
    distances = cosine_similarity(doc_embedding, candidate_embeddings)

    # 각 키워드들 간의 유사도
    distances_candidates = cosine_similarity(candidate_embeddings, 
                                            candidate_embeddings)

    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.
    words_idx = list(distances.argsort()[0][-nr_candidates:])
    words_vals = [candidates[index] for index in words_idx]
    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]

    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산
    min_sim = np.inf
    candidate = None
    for combination in itertools.combinations(range(len(words_idx)), top_n):
        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])
        if sim < min_sim:
            candidate = combination
            min_sim = sim

    return [words_vals[idx] for idx in candidate]

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=10, nr_candidates=10)

max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)

def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):

    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트
    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)

    # 각 키워드들 간의 유사도
    word_similarity = cosine_similarity(candidate_embeddings)

    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # keywords_idx = [2]
    keywords_idx = [np.argmax(word_doc_similarity)]

    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들
    # 만약, 2번 문서가 가장 유사도가 높았다면
    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]
    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]

    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.
    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.
    for _ in range(top_n - 1):
        candidate_similarities = word_doc_similarity[candidates_idx, :]
        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)

        # MMR을 계산
        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)
        mmr_idx = candidates_idx[np.argmax(mmr)]

        # keywords & candidates를 업데이트
        keywords_idx.append(mmr_idx)
        candidates_idx.remove(mmr_idx)

    return [words[idx] for idx in keywords_idx]

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)

mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)

